# 损失函数

![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/loss.png)

损失函数一般是描述单个样本的差异值，或者是一个batch。

代价函数是求总体样本的loss的平均值。

目标函数是由代价函数和正则项构成的，正则项是为了防止过拟合。


## 两种常见的损失函数
![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/loss2.png)

H(p,q)，其中p是真实概率分布，q是输出模型的概率分布。我们是用q去逼近p，所以H(p,q)并不等于H(q,p).分布没有对称关系，不能像MSE一样位置互换。


### 交叉熵

![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/ce.png)

自信息是描述某一事件的信息量。信息熵是用来描述某一事件的不确定度，从图上看出来当概率为0.5的时候，信息熵是最大的，因为无法确定事件是否大概率发生，还是小概率发生。

![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/ce2.png)

相对熵又叫KL-divergence。公式中P是真实分布，Q是模型输出分布。相对熵是用来计算P分布和Q分布之间的差异

公式: 交叉熵 = 信息熵 + 相对熵中，信息熵是个真实分布，是个常数，所以我们就用交叉熵来描述两个分布之间的差异。

![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/ce3.png)

模型没有办法保证输出是一个概率分布的形式，矩阵乘法有时候会得到负数，时候会得到大于1，但是概率不可能是负的，也不可能大于1。于是我们要考虑使用softmax函数


概率有两个性质：
1.概率值是非负的
2.概率之和等于1

交叉熵的好伙伴——softmax函数：将数据变换到符合概率分布的形式

![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/ce4.png)


没有一个适合所有任务的损失函数，损失函数的设计会涉及算法类型，求导是否容易，数据中异常值的分布等问题


更多的损失函数可以到PyTorch官网：
https://pytorch.org/docs/stable/nn.html#loss-functions

函数解读：
https://zhuanlan.zhihu.com/p/61379965