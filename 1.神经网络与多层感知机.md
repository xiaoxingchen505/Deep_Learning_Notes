


# 神经网络与多层感知机

## 人工神经元 Artificial Neural Unit


![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/0.0.png)
![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/0.1.png)
![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/0.2.png)

### 正因为单独的感知机没有办法解决异或问题，我们引入了多层感知机。


## 多层感知机 Multi Layer Perceptron， MLP
![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/0.4.png)


## 激活函数的意义

1.让多层感知机成为真正的多层，否则等于一层

2.引入非线性，使网络可以逼近任意非线性函数 (万能逼近原理, Universal approximator)

![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/0.5.png)

### 激活函数需要具备以下几点性质：

1.连续并可导，允许少数点上不可导，便于利用数值优化的方法来学习网络参数

2.激活函数及其函数要尽可能的简单，有利于提高网络计算效率

3.激活函数的导函数的值域要在合适区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。

![image](https://github.com/xiaoxingchen505/Deep_Learning_Notes/blob/master/images/0.6.png)

RELU在神经网络中更常用的原因是不存在饱和区，不会引起梯度消失。RELU在x=0时，是undefined。也就是说在这个位置不可以求导。这也符合之前我们所说的激活函数允许少数点不可导。


